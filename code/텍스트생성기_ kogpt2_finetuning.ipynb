{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"텍스트 생성기: kogpt2_finetuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2c0cf0d2d5994363bf6d1251b3ef7fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e9c91a7f6d0e48d088034d1740e1cdd1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_65449e15b6e748caa7af1c1b45ecefa0","IPY_MODEL_ea13875096604aa780a41eabb9e33407","IPY_MODEL_56521a8d135e40ce981d7f10552fb389"]}},"e9c91a7f6d0e48d088034d1740e1cdd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65449e15b6e748caa7af1c1b45ecefa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ebbccef125a5466f980003b69d662a06","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92febf788c7441eba1fc0341763ad92c"}},"ea13875096604aa780a41eabb9e33407":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_14041efacf0642ae9869264bd1599a7f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2825034,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2825034,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ec3c61fd3484856942cd483b0f2f0ef"}},"56521a8d135e40ce981d7f10552fb389":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7d9956ef69a8455ba1b2f21c6d29a019","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.83M/2.83M [00:00&lt;00:00, 3.61MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df4353952d144a2da0911ba6febeb87e"}},"ebbccef125a5466f980003b69d662a06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"92febf788c7441eba1fc0341763ad92c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14041efacf0642ae9869264bd1599a7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ec3c61fd3484856942cd483b0f2f0ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d9956ef69a8455ba1b2f21c6d29a019":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"df4353952d144a2da0911ba6febeb87e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4de30d4d7307408fa9479837fb48ce0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_257d174973254a02bb2ee7b3d3398082","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e2eaa979ca6245198d48642a88edcbbc","IPY_MODEL_91d836c49586448c9734839b21b04645","IPY_MODEL_15d13e307ae14173bb8d862c9b09edeb"]}},"257d174973254a02bb2ee7b3d3398082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2eaa979ca6245198d48642a88edcbbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_69fb1530eac844348ea5912df4c7775e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a04cc7631a549f1acb085f948b0b15d"}},"91d836c49586448c9734839b21b04645":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ee1e190e984d4002b482462e69dd5927","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07c2e3a13cea4fd0960cb9d432eb0faa"}},"15d13e307ae14173bb8d862c9b09edeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_18fa2194002b4e40932fa66aa514fa7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.00k/1.00k [00:00&lt;00:00, 18.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_06e3aa85fc6847e188d5ad63fdf4af86"}},"69fb1530eac844348ea5912df4c7775e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1a04cc7631a549f1acb085f948b0b15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee1e190e984d4002b482462e69dd5927":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"07c2e3a13cea4fd0960cb9d432eb0faa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18fa2194002b4e40932fa66aa514fa7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"06e3aa85fc6847e188d5ad63fdf4af86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ebf51224351242e9b5c8e0155c474172":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d2d273cc8f78487293a305e9551b5fce","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bd17ed2f35d04f8ab04462653dedaf0b","IPY_MODEL_334bbe89fef045919ce341d5c05a27b5","IPY_MODEL_c1c470ced22947408703f2207df7164c"]}},"d2d273cc8f78487293a305e9551b5fce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd17ed2f35d04f8ab04462653dedaf0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b22e9a679f504e4ca982c908d45ab09e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_830a8c1d8d43461f89ba4a543f684e33"}},"334bbe89fef045919ce341d5c05a27b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_29a077119fcd410cbf18c2fc7800c443","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":513302779,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":513302779,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da37f631b0034253b430a4ed0b927c00"}},"c1c470ced22947408703f2207df7164c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4ed279ebcbc549199bbede633f8b2b29","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 513M/513M [00:16&lt;00:00, 34.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_949047ef16ec4512b383cf900ac5bed0"}},"b22e9a679f504e4ca982c908d45ab09e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"830a8c1d8d43461f89ba4a543f684e33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29a077119fcd410cbf18c2fc7800c443":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"da37f631b0034253b430a4ed0b927c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ed279ebcbc549199bbede633f8b2b29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"949047ef16ec4512b383cf900ac5bed0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GcDSObPuqPr7"},"source":["#필요한 파일 다운로드"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ciDG7f5qV_E","executionInfo":{"status":"ok","timestamp":1629866884321,"user_tz":-540,"elapsed":9266,"user":{"displayName":"‍김민지[재학 / 동아프리카어전공]","photoUrl":"","userId":"06217683100035561585"}},"outputId":"9777b9be-8206-4970-aceb-4bcef9cb5f7c"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 13.8 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 29.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 34.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fD9BGjOKFuX1","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1629868452926,"user_tz":-540,"elapsed":401,"user":{"displayName":"‍김민지[재학 / 동아프리카어전공]","photoUrl":"","userId":"06217683100035561585"}},"outputId":"06f0b86a-b1b2-4d60-82f8-8751f9924df6"},"source":["import pandas as pd\n","\n","#df = pd.read_csv(\"/content/prep_text.txt\", encoding=\"utf-8\")\n","df=open('/content/prep_text.txt',encoding=\"utf-8\")\n","\n","#text_data = open('/content/brunch.txt', 'w', encoding=\"utf-8\")\n","text_data = open('/content/essay.txt','w',encoding = 'utf-8')\n","\n","\n","texts = df['0']\n","for t in texts:\n","    \n","    if '*' in t:\n","        continue\n","    if '#' in t:\n","        continue\n","    if 'http' in t :\n","        continue\n","    if '@' in t:\n","        continue\n","    if ':' in t:\n","        continue\n","    text_data.write(t + '\\n')\n","text_data.close()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-61c420c2fc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not subscriptable"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":604,"referenced_widgets":["2c0cf0d2d5994363bf6d1251b3ef7fa0","e9c91a7f6d0e48d088034d1740e1cdd1","65449e15b6e748caa7af1c1b45ecefa0","ea13875096604aa780a41eabb9e33407","56521a8d135e40ce981d7f10552fb389","ebbccef125a5466f980003b69d662a06","92febf788c7441eba1fc0341763ad92c","14041efacf0642ae9869264bd1599a7f","5ec3c61fd3484856942cd483b0f2f0ef","7d9956ef69a8455ba1b2f21c6d29a019","df4353952d144a2da0911ba6febeb87e","4de30d4d7307408fa9479837fb48ce0a","257d174973254a02bb2ee7b3d3398082","e2eaa979ca6245198d48642a88edcbbc","91d836c49586448c9734839b21b04645","15d13e307ae14173bb8d862c9b09edeb","69fb1530eac844348ea5912df4c7775e","1a04cc7631a549f1acb085f948b0b15d","ee1e190e984d4002b482462e69dd5927","07c2e3a13cea4fd0960cb9d432eb0faa","18fa2194002b4e40932fa66aa514fa7f","06e3aa85fc6847e188d5ad63fdf4af86","ebf51224351242e9b5c8e0155c474172","d2d273cc8f78487293a305e9551b5fce","bd17ed2f35d04f8ab04462653dedaf0b","334bbe89fef045919ce341d5c05a27b5","c1c470ced22947408703f2207df7164c","b22e9a679f504e4ca982c908d45ab09e","830a8c1d8d43461f89ba4a543f684e33","29a077119fcd410cbf18c2fc7800c443","da37f631b0034253b430a4ed0b927c00","4ed279ebcbc549199bbede633f8b2b29","949047ef16ec4512b383cf900ac5bed0"]},"id":"_CY1a8xDAXr9","executionInfo":{"status":"ok","timestamp":1629868100998,"user_tz":-540,"elapsed":1178616,"user":{"displayName":"‍김민지[재학 / 동아프리카어전공]","photoUrl":"","userId":"06217683100035561585"}},"outputId":"133e9536-4dce-4f15-ace3-a34b722cced8"},"source":["from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import GPT2LMHeadModel\n","from transformers import Trainer, TrainingArguments\n","from transformers import PreTrainedTokenizerFast\n","\n","def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, \n","        mlm=mlm,\n","    )\n","    return data_collator\n","\n","\n","def train(train_file_path,model_name,\n","          output_dir,\n","          overwrite_output_dir,\n","          per_device_train_batch_size,\n","          num_train_epochs,\n","          save_steps):\n","  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n","  train_dataset = load_dataset(train_file_path, tokenizer)\n","  data_collator = load_data_collator(tokenizer)\n","\n","  tokenizer.save_pretrained(output_dir, legacy_format=False)\n","   \n","  model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","  model.save_pretrained(output_dir)\n","\n","  training_args = TrainingArguments(\n","          output_dir=output_dir,\n","          overwrite_output_dir=overwrite_output_dir,\n","          per_device_train_batch_size=per_device_train_batch_size,\n","          num_train_epochs=num_train_epochs,\n","      )\n","\n","  trainer = Trainer(\n","          model=model,\n","          args=training_args,\n","          data_collator=data_collator,\n","          train_dataset=train_dataset,\n","  )\n","      \n","  trainer.train()\n","  trainer.save_model()\n","\n","\n","train_file_path = '/content/prep_text.txt'\n","model_name = 'skt/kogpt2-base-v2'\n","output_dir = './modelv5'\n","overwrite_output_dir = False\n","per_device_train_batch_size =16\n","num_train_epochs = 3\n","save_steps = 500\n","\n","train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c0cf0d2d5994363bf6d1251b3ef7fa0","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4de30d4d7307408fa9479837fb48ce0a","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebf51224351242e9b5c8e0155c474172","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 4399\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 825\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='825' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [825/825 14:29, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>4.276000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["Saving model checkpoint to ./modelv5/checkpoint-500\n","Configuration saved in ./modelv5/checkpoint-500/config.json\n","Model weights saved in ./modelv5/checkpoint-500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./modelv5\n","Configuration saved in ./modelv5/config.json\n","Model weights saved in ./modelv5/pytorch_model.bin\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"em51VgI2AiBK"},"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","\n","def load_model(model_path):\n","    model = GPT2LMHeadModel.from_pretrained(model_path)\n","    return model\n","\n","\n","def load_tokenizer(tokenizer_path):\n","    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n","    return tokenizer\n","\n","\n","def generate_text(sequence, max_length):\n","    model_path = \"./modelv5\"\n","    model = load_model(model_path)\n","    tokenizer = load_tokenizer(model_path)\n","    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n","    final_outputs = model.generate(\n","        ids,\n","        do_sample=True,\n","        max_length=max_length,\n","        pad_token_id=model.config.pad_token_id,\n","        top_k=50,\n","        top_p=0.95,\n","    )\n","    return(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8wbWza0MaiH","executionInfo":{"status":"ok","timestamp":1629868794947,"user_tz":-540,"elapsed":292759,"user":{"displayName":"‍김민지[재학 / 동아프리카어전공]","photoUrl":"","userId":"06217683100035561585"}},"outputId":"46722542-c7e5-43c9-ef7b-60ca16d1431d"},"source":["#제일 기본 코드\n","input = '에펠탑'\n","\n","sequence = input\n","max_len = 300\n","\n","\n","\n","for i in range(3):\n","   \n","\n","    sequence = input\n","    max_len = 1000\n","\n","    print('input :' + sequence)\n","    print('=' * 50)\n","\n","    g = generate_text(sequence, max_len)\n","    g = g.split('<eos>')[0]\n","    g = g.split(',')[2:]\n","    g = ', '.join(g)\n","    input = g.split('\\n')\n","    input = input[-1]\n","    print(g)\n","    \n","\n","    \n","print('=' * 50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["input :에펠탑\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n","loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n","input :\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n","loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["\n","input :\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n"],"name":"stderr"},{"output_type":"stream","text":["\n","==================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfvwH_z6NlKP","executionInfo":{"status":"ok","timestamp":1629869842666,"user_tz":-540,"elapsed":59271,"user":{"displayName":"‍김민지[재학 / 동아프리카어전공]","photoUrl":"","userId":"06217683100035561585"}},"outputId":"6481148c-19fe-41c0-b057-085a495a0fea"},"source":["#keyword에 따른 출력\n","generated_texts = []\n","keywords = ['파리','루브르','에펠탑']\n","\n","for key in keywords:\n","    #input_data = input('키워드 입력 :')\n","    #keywords.append(input_data)\n","    input_data = key.split(' ')\n","    #keywords.append(input)\n","\n","    #max_len = 300\n","    total = ''\n","\n","    for i in input_data:\n","        max_len = 200\n","\n","        #print('input :' + i)\n","        #print('=' * 50)\n","\n","        g = generate_text(','+i, max_len)\n","        g = g.split('<eos>')[0]\n","        g = g.split(',')[2:]\n","        g = ', '.join(g)\n","\n","        t =''\n","        data = g.split('\\n')\n","        data = data[:-1]\n","        for d in data: \n","            t += d + '\\n'\n","\n","        total += '\\n\\t(' + i +')'  + t\n","        \n","    print('=' * 50)\n","    print(\"<generated_text>\")\n","    print(total)\n","    print('=' * 50)\n","    generated_texts.append(total)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n","loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t(파리) 파리가 아닌 프랑스라는 나라의 수도는 프랑스 파리가 아니다\n","파리 지하철 노선의 종점 파리역 몽마르트르 언덕 위 파리역의 몽마르트르 언덕에 오르는 일은 이 두 도시가 내게 또 다른 묘미를 선사해줄 것이라고 믿었다\n","그곳에서 파리지앵들은 매일같이 한식당에서 커피를 마시는다\n","카페 레스토랑에서 한식을 먹다니 파리 여행 중 커피 한 잔도 못 마시고 바로 옆에 카페에 앉아 커피를 마시다 보면 두 도시 모두 색다른 느낌의 파리 여행을 할 수 있었다\n","파리 카페를 거닐다 보면 카페에 앉아있다가도 파리지앵이 만든 도시락에 라떼를 마신다\n","파리지앵과 여행하는 파리지앵에게 파리의 도시는 또 다른 묘미를 선사할 것이다\n","파리는 여전히 여행의 재미에 관심이 많은 도시이다\n","이 도시는 파리지앵이 먹고 자는 곳이다\n","파리는 하루 동안 매일 아침 9시 30분경 문을 열고 파리지앵은 9시까지 커피를 마실 수 있다\n","\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n","loading configuration file ./modelv5/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./modelv5/pytorch_model.bin\n"],"name":"stderr"},{"output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t(루브르)도빌 등의 도시 중심지와 멀지 않은 곳에 위치해 있는데 그중 가장 유명한 지역은 파리의 노트르담 대성당이라 불리는 루브르로 그 역사가 꽤 오래된 곳이다\n","루브르 박물관에는 많은 고대 유물들이 전시되어 있는데 루브르는 고대 그리스 신화에 나오는 오디세우스라는 인물 상대의 이름이 들어있거나 신화 속의 오디세우스와 같은 인물이 조각되어 있고 그 옆에 오디세우스가 앉아있는 곳이 바로 노트르담 대성당\n","그 옛날 그리스의 헤라클레스가 이 곳에서 살았다고 한다\n","그리고 이 곳에서 루브르는 성모 마리아가 탄생할 당시 사람들이 많이 살았던 곳으로 지금도 많은 사람들이 찾으면서 유명하고 있다\n","루브르 박물관으로 입장하면 성당 내부에 있는 에서 성당 내부를 감상하고 지하철에 내려 공원 산책로로 나가면 된다\n","역시 파리의 랜드마크로 불리는 이 성당은 우리나라에도 잘 알려진 곳이다\n","루브르 박물관과 마찬가지로 19세기 말과 20세기 초 사이에 지어진 건물로 원래는 원래 이 성당으로 사용하다가 1917년 재건되어 현재에 이른다\n","\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./modelv5.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./modelv5/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./modelv5/special_tokens_map.json\n","loading file ./modelv5/tokenizer_config.json\n","loading file ./modelv5/tokenizer.json\n"],"name":"stderr"},{"output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t(에펠탑)그리고 에펠탑이 있는 곳에 대한 소개는 아직 없지만 에펠탑이 있는 곳에 대한 소개는 아직 시작되지 않았다\n","오픈 전부터 많은 인파가 모였는데 그때부터 많은 사람들에게 인기를 얻고 있고 아직 막바지인데도 많은 사람들이 에펠탑에 있는 사진들을 올려놓았으면 좋겠다고\n","좋아한다\n","그랬더니 내가 사진을 찍었더니 괜찮아 라는 말이 되었다\n","어쨌거나 사진들을 보니 왠지 뭔지 모르게 에펠탑에 대한 정보가 머릿속에 맴돌았다\n","아무래도 잘 모르겠어서 그저 멍하니 바라보다가 에펠탑에 대해서 다시 한번 생각해보게 되었다\n","어쨌든 이렇게 생각하며 에펠탑을 다시 보았다\n","나는 그 장소에 대해서 다시 돌아다녀서 다시 보게 되었다\n","이제는 추억이 된다\n","파란 하늘에 떠있는 에펠탑\n","이렇게 보면 마치 파리의 겨울처럼 겨울 같았다\n","그리고 아직도 그 장소에 대한 기억이 여전히 그립다\n","아마 그 곳에서 많은 사람들이 지나가고 싶었나 보다\n","\n","==================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"41aR0pxnjce7"},"source":["save2= pd.DataFrame({'keywords':keywords, 'texts': generated_texts})\n","save2.to_csv('daily_test.csv', encoding = 'utf-8-sig')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkjZ6jlDpc4x","outputId":"6d7190c2-9058-4f40-d697-6526a0494a7f"},"source":["#마지막 문장이 gpt입력으로 들어감\n","generated_texts = []\n","keywords = ['파리','밤','산책']\n","\n","for key in keywords:\n","    #input_data = input('키워드 입력 :')\n","    #keywords.append(input_data)\n","    sequence = key\n","    #keywords.append(input)\n","\n","    #max_len = 300\n","    total = ''\n","\n","    for i in input_data:\n","        max_len = 100\n","\n","        #print('input :' + i)\n","        #print('=' * 50)\n","        for j in range(3):\n","            g = generate_text(sequence, max_len)\n","            g = g.split('<eos>')[0]\n","            g = g.split(',')[2:]\n","            g = ', '.join(g)\n","\n","            t =''\n","            a = g.replace('.', '.&&')\n","            a = a.split('&&')\n","            data = a[:-1]\n","            if len(data)> 0:\n","                sequence = data[-1]\n","            for d in data: \n","                t += d + '\\n'\n","\n","            total += '\\n\\t' + t\n","            \n","    print('=' * 50)\n","    print(\"<generated_text>\")\n","    print(total)\n","    print('=' * 50)\n","    generated_texts.append(total)"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n"]},{"name":"stdout","output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t 그리고 에펠탑을 볼 수 있는 숙소로 추천한다.\n","\n","아침 일찍 숙소에 들어온 덕분인지 파리의 아침은 제법 쌀쌀했다.\n","\n","오늘의 메인은 아침식사로 나오는 콩코드 광장에서 한식이다.\n","\n","한식은 집에서 만드는 김치 보다 집에서 만드는 김치 맛이 훨씬 좋다.\n","\n","혼자서 먹는데도 질리지 않고 좋으며 혼자 먹어도 질리지 않다.\n","\n","\t 그리고 소스를 곁들인 아침 식사 대용 아침 메뉴.\n",".\n","\n","\t\n","\t 그땐 이미 그 모든 것이 꿈틀대기 시작했었다.\n","\n","이번 파리 여행에서도 그랬고 그때도 그랬고.\n"," 저게 지금 사진기 속에 나온 모습이잖아.\n","\n","사실,  그 당시만 해도 내 사진을 보고 싶다는 욕망은 있었는데,  그 시절 내 사진을 보면서 이게 웬 떡이란다는 생각을 했었다.\n","\n","\t  그 시절 내 사진을 보면서 이게 웬 떡이란다는 생각을 했었다.\n",", \n","더욱이 파리여행 중 파리에 간 날은 비행기가 저녁 즈음에 도착하는 바람에 더더욱 내가 보고 싶었고, \n","나중에 기억을 되살리면 될 거 같다.\n","\n","그래서,  그때는 나의 사진과 여행 후기를 함께 쓰자고 제안했다.\n","\n","정말,  그때는 혼자였다.\n","\n","\t.\n",".\n","\n","처음에는 엄마를 모시고 파리여행을 갔던 적이 있습니다.\n","\n","처음에는 그냥 혼자였기 때문에 혼자였기 때문에 엄마의 집을 드나들었을 뿐이었지만 이제는 엄마의 마음도 나중엔 내 마음 같이 느껴졌습니다.\n","\n","일도 제법 크게 느긋하게 시작했으니까요.\n","\n","처음엔 엄마와 눈을 맞춰야겠다는 생각이 들었는데 사실 엄마는 전혀 그런 계획이 없었었죠.\n","\n","\t\n","\t 그날의 저녁은 빵집 앞에 앉아서 간단히 브런치를 즐기던 엄마와 나는 잠시 눈시울을 붉혔죠.\n","\n","이번에는 엄마의 안심이 필요했죠.\n","\n","우리의 여행의 후반부에선 다시 엄마와 눈을 맞춰야겠다는 생각이 들었거든요.\n","\n","\t\n","마음 같아서는 여행을 다녀오기로 했어요.\n","\n","엄마와 여행을 함께하기로 했으니 엄마는 꼭 좋은 곳에 다녀오라고 격려해 주셨어요.\n","\n","그게 어느 정도 안 됐을 때, \n","이런 말을 들으니 저도 마음이 찡한 게 사실인데요.\n","\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n"]},{"name":"stdout","output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t\n","\t 거리의 악사들도 함께 나와 거리로 쏟아져 나왔다.\n","\n","그런데 이상하게도 몽마르트르 언덕에 있던 많은 사람들이 거리를 비집고 들어온 것이다.\n","\n","그리고 그 모습을 보며 나도 모르게'어째서 길을 걸은 거지?'하는 생각이 들었다.\n","\n","\t 파리에 도착했을 때,  우연히 마주친 반가움과 설레는 감정을.\n",".\n",".\n",".\n",".\n"," 그때,  나의 파리.\n",".\n",".\n",".\n",".\n","\n","\t 프랑스 스케치 - 파리 샹젤리제 거리'편을 시작으로 여러분과 프랑스 여행을 떠납니다.\n","\n","다시 파리에 와서 몽마르트르 언덕과 에펠탑 그리고 센 강의 낭만을 즐길 수 있다면,  파리를 가장 아름답게 만들 수는 없을까요?\n","아마도 그때,  또 그땐'지금이 파리에서 가장 아름다운'것 같으니까요.\n","\n","\t  또 그땐'지금이 파리에서 가장 아름다운'것 같으니까요.\n",", \n","그렇게 몽마르트르 언덕에 올라 에펠탑의 야경과 파리의 밤을 감상하며,  내일 다시 파리에 오겠습니다.\n","\n","\t  내일 다시 파리에 오겠습니다.\n",", \n","마지막으로,  센강을 따라 천천히 샹젤리제 거리를 걸으며 여러분께 다시 인사를 드리겠습니다!\n","| skiprance |\n","| 르 셀렉트 | 센강 끝자락 | 파리 | 도심을 돌아보는 여행 방법은 이렇습니다.\n","\n","\t  센강을 따라 천천히 샹젤리제 거리를 걸으며 여러분께 다시 인사를 드리겠습니다!\n","| skiprance |\n","| 르 셀렉트 | 센강 끝자락 | 파리 | 도심을 돌아보는 여행 방법은 이렇습니다.\n","\n","\t\n","\t\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n","loading configuration file ./models2/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.9.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file ./models2/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file ./models2/added_tokens.json. We won't load it.\n","loading file None\n","loading file ./models2/special_tokens_map.json\n","loading file ./models2/tokenizer_config.json\n","loading file ./models2/tokenizer.json\n"]},{"name":"stdout","output_type":"stream","text":["==================================================\n","<generated_text>\n","\n","\t 산책을 하며 살고 있었다.\n","\n","조용히 잘 보내고 싶은 마음에 밖으로 나가보니 어느새 이른 겨울이라서 길을 찾지 못해 헤매다가 숙소에 도착했다.\n","\n","숙소에 들어서니 따뜻한 바람이 나를 기분 좋게 했다.\n","\n","저녁 먹고 싶은 데가 몇 군데 있어서 나가니 조곤조곤 비가 내리기 시작했다.\n","\n","그냥 우산을 쓰고 왔는데 비가 너무 많이 온다고 해서 우산을 사서 간거다.\n","\n","\t 비가 많이 오니 짐을 쌌다가 비가 너무 많이 와서 어쩔 수 없이 우산을 뺐다.\n","\n","\t\n","\t 호선 바스티유 역에서 호선으로 갈아타고,  호선 롱비치에 다시 지하철 역으로 갈아탔다.\n","\n","\t 몽마르트르 언덕은 정말 좋았다.\n","\n","그리고 지금 몽마르트르 언덕에서 가장 높은 곳에 위치한 뤽상부르 정원에서는 파리 여행의 시작을 알리는 불꽃이 요란하게 불었다.\n","\n","우리나라에는 파리 외에도 대운하를 두고 여러 곳들이 공사 중이었다.\n","\n","\t 루브르 박물관,  라데팡스,  퐁피두 센터,  에펠 타워,  퐁피두 센터,  샹젤리제 거리,  에펠 타워,  베르사유 궁전도 있다.\n","\n","또,  파리 시내 전경을 감상하면서 루브르 박물관의 중요 작품을 감상하면서 파리 시내 전경을 바라볼 수 있는 전망대도 있다.\n","\n","\t,  루브르 박물관 외에도 센 강을 가로지르는 센 강 센 강의 유람선,  노트르담 성당,  생트 샤펠에서 도보 분 내에 이동할 수 있는 역세권도 함께 위치해 있어 파리의 모든 곳을 이동할 수 있다.\n",",  대중교통 이용이 편리하고,  루브르 박물관과 인접해 있어 파리의 모든 곳을 자유롭게 이동할 수 있다.\n","\n","\t  루브르 박물관과 인접해 있어 파리의 모든 곳을 자유롭게 이동할 수 있다.\n",", \n","조식 및 각종 기념품도 준비되어 있다.\n","\n","가격은 박 만 원대.\n"," 라 비에슈 호스테이트 객실에서 에펠탑을 볼 수 있는 투리즈메 바가 무료 제공된다.\n","\n","입구와 테라스도 인기가 있다.\n","\n","\t 식기세척기 등 간단한 조리가 가능한 제품을 구입하자.\n","\n","식기세척기,  전자레인지,  와인병 등 주방용품을 합리적인 가격에 구입하자.\n","\n","조식은 뷔페와 비슷하지만 뷔페보다는 저렴한 가격이며 디너가 가능하기에 뷔페보다 디너를 추천하는 편이다.\n","\n","==================================================\n"]}]},{"cell_type":"code","metadata":{"id":"vJLakiQSvcUd"},"source":["for idx in range(len(generated_texts)):\n","    temp = ''\n","    gen = generated_texts[idx].split('\\n')\n","    for s in gen:\n","        s = s.strip()\n","        temp += s +'\\n'\n","    generated_texts[idx]= temp \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLZ0dya3vqBt"},"source":["save2= pd.DataFrame({'keywords':keywords, 'texts': generated_texts})\n","save2.to_csv('daily_input_connected.csv', encoding = 'utf-8-sig')"],"execution_count":null,"outputs":[]}]}