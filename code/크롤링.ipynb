{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "크롤링.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ-mKVpBii2H"
      },
      "source": [
        "# 브런치 크롤링 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLW9JHbHib_L"
      },
      "source": [
        "\n",
        "#모듈 임포트\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "\n",
        "#최대로 스크롤 내리는 함수 정의\n",
        "def doScrollDown(whileSeconds):\n",
        "    start = datetime.datetime.now()\n",
        "    end = start + datetime.timedelta(seconds=whileSeconds)\n",
        "    while True:\n",
        "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
        "        time.sleep(0.5)\n",
        "        if datetime.datetime.now() > end:\n",
        "            break\n",
        "\n",
        "#브라우저 설정 및 새 브라우저 열기\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--start-fullscreen')\n",
        "driver = webdriver.Chrome('./chromedriver', options = options) \n",
        "driver.maximize_window()\n",
        "driver.get('https://brunch.co.kr/keyword/프랑스여행')\n",
        "\n",
        "#50초 동안 스크롤 내리기\n",
        "doScrollDown(50)\n",
        "\n",
        "#글 제목과 본문을 저장할 리스트 생성\n",
        "title_list =[]\n",
        "article_list = []\n",
        "\n",
        "#제목을 수집해 title_list에 저장 -> 클릭해 새탭에서 열기 -> 본문 글 수집 -> 탭 종료 및\n",
        "#검색결과 화면으로 돌아감 -> 반복\n",
        "title = driver.find_elements_by_class_name(\"tit_subject\")\n",
        "for k in notebook.tqdm(title, desc = 'progress bar'):\n",
        "    title_list.append(k.text)\n",
        "    webdriver.ActionChains(driver).key_down(Keys.CONTROL).click(k).key_up(Keys.CONTROL).perform()\n",
        "    driver.switch_to.window(driver.window_handles[-1])\n",
        "    \n",
        "    article = driver.find_elements_by_class_name(\"item_type_text\")\n",
        "    join_article_list = []\n",
        "    for j in article:\n",
        "        join_article_list.append(j.text)\n",
        "        fully_combined = \" \".join(map(str, join_article_list))\n",
        "    article_list.append(fully_combined)\n",
        "    time.sleep(1)\n",
        "    driver.close()\n",
        "    driver.switch_to.window(driver.window_handles[0])\n",
        "    doScrollDown(1)\n",
        "\n",
        "#데이터프레임으로 정리, export\n",
        "brunch_df = pd.DataFrame(data = {'Title' : title_list, 'Article' : article_list})\n",
        "brunch_df.to_csv('brunch_df.csv', index=False, encoding=\"utf-8-sig\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFMZwMz3ih0X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8f0Hx-X3ZX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n16PD0bui1tv"
      },
      "source": [
        "#블로그 크롤링 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03jL1v8Wix07"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib.parse import quote\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "def get_posts(query, page_num):\n",
        "    url_query = quote(query)  # 검색어 형태로 변경해주는 함수\n",
        "\n",
        "    #url\n",
        "    url = 'https://search.naver.com/search.naver?query=' + url_query + '&nso=&where=blog&sm=tab_opt'\n",
        "\n",
        "    #data 저장\n",
        "    blog_df = pd.DataFrame(columns=('Title', 'Date', 'Blog Url', 'Post'))\n",
        "    idx = 0\n",
        "    # 스크롤을 위한 driver선언\n",
        "    driver = webdriver.Chrome()\n",
        "    driver.get(url=url)\n",
        "\n",
        "    # 입력한 페이지 수 만큼 스크롤\n",
        "    for i in range(page_num):\n",
        "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "        time.sleep(1)\n",
        "        # print(i)\n",
        "\n",
        "    # 크롤링 과정 : 검색화면 - 글 제목, 작성 날짜(중요x라서 형식에 맞게 변환x),url 스크래핑\n",
        "    # url-블로그 포스터 전문 확인 가능\n",
        "    # 크롤링 방지용 iframe 속으로 들어가서, post의 text만 크롤링하여 저장\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    posts = soup.find_all('div', {'class': 'total_wrap api_ani_send'})  # 검색화면의 블로그 목록\n",
        "\n",
        "    for post in posts:\n",
        "        # post 제목, 날짜\n",
        "        title = post.select_one('.api_txt_lines.total_tit').text\n",
        "        date = post.find('span', {'class': 'sub_time sub_txt'}).get_text()\n",
        "\n",
        "        # url->블로그 창 연결\n",
        "        post_url = post.find('a', {'class': 'api_txt_lines total_tit'}).get('href')\n",
        "\n",
        "        try:\n",
        "            post_link = urllib.request.urlopen(post_url).read()\n",
        "            # post_link = req.urlopen(post_url).read()\n",
        "            post_html = BeautifulSoup(post_link, 'html.parser')\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # iframe src를 통해 원본 접근\n",
        "        for main_frame in post_html.select(\"iframe#mainFrame\"):\n",
        "            frame_url = \"https://blog.naver.com\" + main_frame.get('src')\n",
        "            post_text = urllib.request.urlopen(frame_url).read()\n",
        "            post_html = BeautifulSoup(post_text, 'html.parser')\n",
        "            post_content_text = \"\"\n",
        "\n",
        "            for post_content in post_html.find_all('div', {'class': 'se-main-container'}):\n",
        "                post_content_text = post_content.get_text()\n",
        "\n",
        "            # 형식이 다른 블로그 존재, 예외처리\n",
        "            if post_content_text == \"\":\n",
        "                for post_content in post_html.find_all('div', {'id': 'postViewArea'}):\n",
        "                    post_content_text = post_content.get_text()\n",
        "            post_content_text = post_content_text.replace('\\n', '')\n",
        "        blog_df.loc[idx] = [title, date, post_url, post_content_text]\n",
        "        idx += 1\n",
        "    return blog_df\n",
        "\n",
        "query = input(\"검색: \")\n",
        "page_num = int(input(\"page 수: \"))  # 한 페이지당 약 30개\n",
        "blog_df= get_posts(query, page_num)\n",
        "print(blog_df.head())\n",
        "blog_df.to_csv('blog_data_info.csv',encoding= 'utf-8-sig')\n",
        "print(\"끝\")\n",
        "```"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}